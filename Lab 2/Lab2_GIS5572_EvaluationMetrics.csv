Evaluation Approach / Metric,Appropriate Data Types,Mathematic Definition (if applicable),ArcPy function (if applicable),How to do in Python,What metrics is this approach similar / different to?,Sources
Confusion Matrix,Array,"Table with TP, FP, TN, FN",none,`from sklearn.metrics import confusion_matrix`,none,https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html
Accuracy,Float,(TP + TN) / (TP + TN + FP + FN),none,`from sklearn.metrics import accuracy_score`,Similar to precision and recall but more general.,https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html
Precision,Float,TP / (TP + FP),none,`from sklearn.metrics import precision_score`,Similar to accuracy but focuses on positive cases.,https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html
Recall,Float,TP / (TP + FN),none,`from sklearn.metrics import recall_score`,Similar to accuracy but focuses on actual positives.,https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html
True Positives,Integer,Correctly predicted positive cases,none,`(y_pred == 1) & (y_true == 1)`,Related to recall and precision.,https://en.wikipedia.org/wiki/Confusion_matrix
False Positives,Integer,Incorrectly predicted positive cases,none,`(y_pred == 1) & (y_true == 0)`,Related to false negatives.,https://en.wikipedia.org/wiki/Confusion_matrix
Receiver Operator Characteristic (ROC) Curve and Area Under the Curve,Float,Graph plotting TPR vs. FPR; AUC measures overall performance,none,"`from sklearn.metrics import roc_curve, auc`",AUC is a summary measure of the ROC curve performance.,https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html
R-squared,Float,1 - (Residual Sum of Squares / Total Sum of Squares),"arcpy.sa (spatial analyst), import *. r² = regression_results.getOutput(""RSquared"")",`from sklearn.metrics import r2_score`,Similar to adjusted R² but does not penalize complexity.,https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html
Adjusted R-Squared,Float,1 - ((1 - R²) * (n-1) / (n-p-1)),none,"`import statsmodels.api as sm; sm.OLS(y, X).fit().rsquared_adj`",Similar to R² but adjusts for number of predictors.,https://www.statsmodels.org/stable/generated/statsmodels.regression.linear_model.OLS.html
Root Mean Square Error,Float,sqrt(mean((observed - predicted)^2)),none,`from sklearn.metrics import mean_squared_error`,Similar to MAE but penalizes large errors more.,https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html
Mean Absolute Error,Float,mean(abs(observed - predicted)),none,`from sklearn.metrics import mean_absolute_error`,Similar to RMSE but does not square errors.,https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html
Residual Standard Error,Float,sqrt(sum(residuals^2) / (n - p - 1)),none,`import numpy as np; np.sqrt(np.sum(residuals**2) / (n - p - 1))`,Similar to RMSE but adjusts for degrees of freedom.,https://en.wikipedia.org/wiki/Residual_standard_error
Akaike’s Information Criterion (AIC),Float,-2 * log-likelihood + 2k,none,"`sm.OLS(y, X).fit().aic`",Used for model comparison like BIC but penalizes complexity differently.,https://en.wikipedia.org/wiki/Akaike_information_criterion
Bayesian Information Criterion (BIC),Float,-2 * log-likelihood + k * log(n),none,"`sm.OLS(y, X).fit().bic`",Used for model comparison like AIC but penalizes complexity more.,https://en.wikipedia.org/wiki/Bayesian_information_criterion